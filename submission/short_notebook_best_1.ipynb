{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2f2bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn part\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler \n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# other models\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# other\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2efacb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c75277",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5838a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_lat_lon(df):\n",
    "    df['lat_processed'] = df.lat * 11.112\n",
    "    df['lon_processed'] = df.lon * 6.4757\n",
    "    return df\n",
    "\n",
    "def fix_outliers(df):\n",
    "    return df[df.revenue != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374a5e5",
   "metadata": {},
   "source": [
    "# Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af157c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_year(df):\n",
    "    return df[df.year == 2016]\n",
    "\n",
    "\n",
    "def create_population_repartition(df_grunnkrets_house_pers):\n",
    "    tmp = df_grunnkrets_house_pers[[\n",
    "            'grunnkrets_id',\n",
    "            'couple_children_0_to_5_years',\n",
    "            'couple_children_6_to_17_years',\n",
    "            'couple_children_18_or_above',\n",
    "            'couple_without_children',\n",
    "            'single_parent_children_0_to_5_years',\n",
    "            'single_parent_children_6_to_17_years',\n",
    "            'single_parent_children_18_or_above',\n",
    "            'singles'\n",
    "        ]]\n",
    "    return tmp.rename({column: 'hp_' + column for column in tmp.columns if column != 'grunnkrets_id'}, axis=1)\n",
    "\n",
    "\n",
    "def create_income_repartition(df_grunnkrets_income_house):\n",
    "    tmp = df_grunnkrets_income_house[[\n",
    "            'grunnkrets_id',\n",
    "            'all_households',\n",
    "            'singles',\n",
    "            'couple_without_children',\n",
    "            'couple_with_children',\n",
    "            'other_households',\n",
    "            'single_parent_with_children'\n",
    "            ]]\n",
    "    return tmp.rename({column: 'ih_' + column for column in tmp.columns if column != 'grunnkrets_id'}, axis=1)\n",
    "\n",
    "\n",
    "def create_grunnkret_geodata(df_grunnkrets_stripped):\n",
    "    return df_grunnkrets_stripped[[\n",
    "                            'grunnkrets_id',\n",
    "                            'area_km2',\n",
    "                            'municipality_name'\n",
    "                            ]]\n",
    "\n",
    "\n",
    "def create_hierarchy(df_plaace_hierarchy):\n",
    "    return df_plaace_hierarchy[[\n",
    "                            'plaace_hierarchy_id',\n",
    "                            'lv1_desc',\n",
    "                            'lv2_desc',\n",
    "                            'lv3_desc'\n",
    "                            ]]\n",
    "\n",
    "\n",
    "def create_population_age(df_grunnkrets_age_dist):\n",
    "    df_grunnkrets_population = df_grunnkrets_age_dist.loc[:, ['grunnkrets_id']]\n",
    "    df_grunnkrets_population['total_population'] = df_grunnkrets_age_dist.iloc[:,2:92].sum(axis=1)\n",
    "    df_grunnkrets_population['youngs'] = df_grunnkrets_age_dist.iloc[:, 2:20].sum(axis=1)\n",
    "    df_grunnkrets_population['adults'] = df_grunnkrets_age_dist.iloc[:, 21:64].sum(axis=1)\n",
    "    df_grunnkrets_population['seniors'] = df_grunnkrets_age_dist.iloc[:, 65:92].sum(axis=1)\n",
    "    return df_grunnkrets_population\n",
    "\n",
    "\n",
    "def find_closest_bus_stop(df, df_bus_stops):\n",
    "    \"\"\"\n",
    "    Combine the training data with the bus stop data by finding :\n",
    "    - the closest bus stop from the store (create a feature the minimal distance then)\n",
    "    - the mean distance of every bus stop in 1km radius\n",
    "    for each category of bus stop\n",
    "    \"\"\"\n",
    "    df['lat_processed'] = df.lat_processed * 10\n",
    "    df['lon_processed'] = df.lon_processed * 10\n",
    "\n",
    "    categories = ['Mangler viktighetsniv√•',\n",
    "                  'Standard holdeplass',\n",
    "                  'Lokalt knutepunkt',\n",
    "                  'Nasjonalt knutepunkt',\n",
    "                  'Regionalt knutepunkt',\n",
    "                  'Annen viktig holdeplass']\n",
    "\n",
    "    new_bs_features = pd.DataFrame(df.store_id)\n",
    "\n",
    "    df_bus_tmp = df_bus_stops.loc[:, ['busstop_id']]\n",
    "    df_bus_tmp[['lon_processed', 'lat_processed']] = df_bus_stops['geometry'].str.extract(\n",
    "        r'(?P<lat>[0-9]*[.]?[0-9]+)\\s(?P<lon>[0-9]*[.]?[0-9]+)', expand=True)\n",
    "    df_bus_tmp['lon_processed'] = pd.to_numeric(df_bus_tmp['lon_processed']) * 6.4757 * 10  # value in km\n",
    "    df_bus_tmp['lat_processed'] = pd.to_numeric(df_bus_tmp['lat_processed']) * 11.112 * 10  # value in km\n",
    "\n",
    "    mat = cdist(df_bus_tmp[['lat_processed', 'lon_processed']], df[['lat_processed', 'lon_processed']], metric='euclidean')\n",
    "    correlation_dist = pd.DataFrame(mat, index=df_bus_tmp['busstop_id'], columns=df['store_id'])\n",
    "    new_bs_features = pd.merge(new_bs_features,\n",
    "                               pd.DataFrame(correlation_dist.min(),\n",
    "                                            columns=['BS_closest']),\n",
    "                               on='store_id', how='left')\n",
    "    new_bs_features = pd.merge(new_bs_features,\n",
    "                               pd.DataFrame(correlation_dist[correlation_dist < 1].mean(),\n",
    "                                            columns=['BS_mean_1km']),\n",
    "                               on='store_id', how='left')\n",
    "    new_bs_features = pd.merge(new_bs_features,\n",
    "                               pd.DataFrame(correlation_dist[correlation_dist < 0.5].count(),\n",
    "                                                columns=['number_BS_500m']), on='store_id')\n",
    "\n",
    "    for category in categories:\n",
    "        df_bus_tmp = df_bus_stops[df_bus_stops['importance_level'] == category].loc[:, ['busstop_id']]\n",
    "        df_bus_tmp[['lon_processed', 'lat_processed']] = df_bus_stops['geometry'].str.extract(\n",
    "                                                                r'(?P<lat>[0-9]*[.]?[0-9]+)\\s(?P<lon>[0-9]*[.]?[0-9]+)',\n",
    "                                                                expand=True)\n",
    "        df_bus_tmp['lon_processed'] = pd.to_numeric(df_bus_tmp['lon_processed']) * 6.4757 * 10    # value in km\n",
    "        df_bus_tmp['lat_processed'] = pd.to_numeric(df_bus_tmp['lat_processed']) * 11.112 * 10    # value in km\n",
    "\n",
    "        mat = cdist(df_bus_tmp[['lat_processed', 'lat_processed']], df[['lat_processed', 'lon_processed']], metric='euclidean')\n",
    "        correlation_dist = pd.DataFrame(mat, index=df_bus_tmp['busstop_id'], columns=df['store_id'])\n",
    "        new_bs_features = pd.merge(new_bs_features,\n",
    "                                   pd.DataFrame(correlation_dist.min(),\n",
    "                                                columns=['BS_closest_' + category.lower().replace(' ', '_')]),\n",
    "                                   on='store_id', how='left')\n",
    "        new_bs_features = pd.merge(new_bs_features,\n",
    "                                   pd.DataFrame(correlation_dist[correlation_dist < 1].mean(),\n",
    "                                                columns=['BS_mean_1km_'+category.lower().replace(' ', '_')]),\n",
    "                                   on='store_id', how='left')\n",
    "    return new_bs_features.fillna(0)\n",
    "\n",
    "\n",
    "def fix_municipalities(current_df):\n",
    "    # Get the rows with missing municipality\n",
    "    df_missing_mun = current_df[current_df[\"municipality_name\"].isna()]\n",
    "    # Create a copy of the current df and drop row where mun = NaN + Reset index\n",
    "    current_df_copy = current_df.copy().dropna(subset=['municipality_name'])\n",
    "    current_df_copy = current_df_copy.reset_index(drop=True)\n",
    "    # For each missing municipality\n",
    "    for index, row in df_missing_mun.iterrows():\n",
    "        # Create a df with the the difference with the loc of the current store and all the others stores\n",
    "        tmp_df = pd.concat([current_df_copy.loc[:, [\"lat_processed\"]] - row.lat_processed,\n",
    "                            current_df_copy.loc[:, [\"lon_processed\"]] - row.lon_processed], axis=1)\n",
    "        # Find the idx of the one with the smallest error (the closest from the other)\n",
    "        idx = np.argmin(np.linalg.norm(tmp_df.to_numpy(), axis=1))\n",
    "        # Retrieve the municipality of the closest one and input it in the missing one\n",
    "        current_df.loc[index, \"municipality_name\"] = current_df_copy.loc[idx, \"municipality_name\"]\n",
    "    return current_df\n",
    "\n",
    "\n",
    "def keep_only_use_features(current_df, features):\n",
    "    return current_df.loc[:, features]\n",
    "\n",
    "\n",
    "def drop_non_use_features(current_df):\n",
    "    return current_df.drop([\n",
    "                        'year',\n",
    "                        'store_id',\n",
    "                        'store_name',\n",
    "                        'address',\n",
    "                        'sales_channel_name',\n",
    "                        'chain_name',\n",
    "                        'mall_name',\n",
    "                        'plaace_hierarchy_id',\n",
    "                        'lv1_desc',\n",
    "                        'lv2_desc',\n",
    "                        'lv3_desc',\n",
    "                        'municipality_name',\n",
    "                        ], axis=1)\n",
    "\n",
    "\n",
    "def label_uniformier(array_train, array_test):\n",
    "    \"\"\"\n",
    "    Take the unique values from the train and test part to combine it in a single array.\n",
    "    Useful to fit the label encoder and don't do a mess during the transform (previously fit_transform that was confusing)\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = np.asarray(list(array_train.unique()) + list(set(array_test.unique()) - set(array_train.unique())))\n",
    "    label_encoder.fit(labels)\n",
    "    return label_encoder\n",
    "\n",
    "\n",
    "def encode_feature(df_train, df_test, feature_name):\n",
    "    le = label_uniformier(df_train[feature_name], df_test[feature_name])\n",
    "    df_train['encoded_' + feature_name] = le.transform(df_train[feature_name])\n",
    "    df_test['encoded_' + feature_name] = le.transform(df_test[feature_name])\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def fill_nan_mean(current_df):\n",
    "    return current_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "\n",
    "\n",
    "def store_secret_feature(df):\n",
    "    tmp_df = df.loc[:, ['store_id', ]]\n",
    "    tmp_df[['SI_p1', 'SI_p2', 'SI_p3']] = df['store_id'].str.extract(r'(?P<p1>[0-9]+)-+(?P<p2>[0-9]+)-+(?P<p3>[0-9]+)', expand=True)\n",
    "    tmp_df['SI_all'] = tmp_df[['SI_p1', 'SI_p2', 'SI_p3']].stack().groupby(level=0).apply(''.join)\n",
    "    tmp_df[['SI_p1', 'SI_p2', 'SI_p3']] = tmp_df[['SI_p1', 'SI_p2', 'SI_p3']].apply(pd.to_numeric)\n",
    "    tmp_df['SI_all'] = tmp_df['SI_all'].astype('float')\n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "def lat_lon_precisionless(df):\n",
    "    tmp_df = df.loc[:, ['store_id', 'lat', 'lon']]\n",
    "    tmp_df[['lat', 'lon']] = tmp_df[['lat', 'lon']]\n",
    "    tmp_df[['lat_reduced', 'lon_reduced']] = tmp_df[['lat', 'lon']].astype('int')\n",
    "    return tmp_df[['store_id', 'lat_reduced', 'lon_reduced']]\n",
    "\n",
    "\n",
    "def scale_values(df_train, df_test, Y_train):\n",
    "    scaler = StandardScaler()\n",
    "    df_train[df_train.columns] = scaler.fit_transform(df_train)\n",
    "    df_test[df_train.columns] = scaler.transform(df_test)\n",
    "\n",
    "    Y_train = np.log10(Y_train + 1)\n",
    "\n",
    "    return df_train, df_test, Y_train\n",
    "\n",
    "\n",
    "def extract_revenue(df_train):\n",
    "    return df_train.loc[:, ['revenue', ]], df_train.drop(['revenue'], axis=1)\n",
    "\n",
    "\n",
    "def features_engineering(df_train, df_test):\n",
    "    df_bus_stops = pd.read_csv(\"data/busstops_norway.csv\")\n",
    "    df_grunnkrets_age_dist = pd.read_csv(\"data/grunnkrets_age_distribution.csv\")\n",
    "    df_grunnkrets_house_pers = pd.read_csv(\"data/grunnkrets_households_num_persons.csv\")\n",
    "    df_grunnkrets_income_house = pd.read_csv(\"data/grunnkrets_income_households.csv\")\n",
    "    df_grunnkrets_stripped = pd.read_csv(\"data/grunnkrets_norway_stripped.csv\")\n",
    "    df_plaace_hierarchy = pd.read_csv(\"data/plaace_hierarchy.csv\")\n",
    "\n",
    "    df_grunnkrets_stripped = fix_year(df_grunnkrets_stripped)\n",
    "    df_grunnkrets_age_dist = fix_year(df_grunnkrets_age_dist)\n",
    "    df_grunnkrets_house_pers = fix_year(df_grunnkrets_house_pers)\n",
    "    df_grunnkrets_income_house = fix_year(df_grunnkrets_income_house)\n",
    "\n",
    "    df_train = pd.merge(df_train, store_secret_feature(df_train), how=\"left\", on=\"store_id\")\n",
    "    df_test = pd.merge(df_test, store_secret_feature(df_test), how=\"left\", on=\"store_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, lat_lon_precisionless(df_train), how=\"left\", on=\"store_id\")\n",
    "    df_test = pd.merge(df_test, lat_lon_precisionless(df_test), how=\"left\", on=\"store_id\")\n",
    "    df_train['latxlat'] = df_train['lat_reduced']*df_train['lon_reduced']\n",
    "    df_test['latxlat'] = df_test['lat_reduced']*df_test['lon_reduced']\n",
    "\n",
    "    df_train = pd.merge(df_train, create_population_repartition(df_grunnkrets_house_pers), how=\"left\", on=\"grunnkrets_id\")\n",
    "    df_test = pd.merge(df_test, create_population_repartition(df_grunnkrets_house_pers), how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, create_income_repartition(df_grunnkrets_income_house), how=\"left\",on=\"grunnkrets_id\")\n",
    "    df_test = pd.merge(df_test, create_income_repartition(df_grunnkrets_income_house), how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, create_hierarchy(df_plaace_hierarchy), how=\"left\", on=\"plaace_hierarchy_id\")\n",
    "    df_test = pd.merge(df_test, create_hierarchy(df_plaace_hierarchy), how=\"left\", on=\"plaace_hierarchy_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, create_grunnkret_geodata(df_grunnkrets_stripped), how=\"left\", on=\"grunnkrets_id\")\n",
    "    df_test = pd.merge(df_test, create_grunnkret_geodata(df_grunnkrets_stripped), how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, create_population_age(df_grunnkrets_age_dist), how=\"left\", on=\"grunnkrets_id\")\n",
    "    df_test = pd.merge(df_test, create_population_age(df_grunnkrets_age_dist), how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "    df_train[\"population_density\"] = df_train[\"total_population\"] / df_train[\"area_km2\"]\n",
    "    df_test[\"population_density\"] = df_test[\"total_population\"] / df_test[\"area_km2\"]\n",
    "\n",
    "    df_train = pd.merge(df_train, find_closest_bus_stop(df_train, df_bus_stops), how=\"left\", on=\"store_id\")\n",
    "    df_test = pd.merge(df_test, find_closest_bus_stop(df_test, df_bus_stops), how=\"left\", on=\"store_id\")\n",
    "\n",
    "    df_train['mall_name'] = df_train['mall_name'].fillna('0')\n",
    "    df_test['mall_name'] = df_test['mall_name'].fillna('0')\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'mall_name')\n",
    "\n",
    "    df_train['chain_name'] = df_train['chain_name'].fillna('0')\n",
    "    df_test['chain_name'] = df_test['chain_name'].fillna('0')\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'chain_name')\n",
    "\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'sales_channel_name')\n",
    "\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'lv3_desc')\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'lv2_desc')\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'lv1_desc')\n",
    "\n",
    "    df_train = fix_municipalities(df_train)\n",
    "    df_test = fix_municipalities(df_test)\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'municipality_name')\n",
    "\n",
    "    Y_train, df_train = extract_revenue(df_train)\n",
    "\n",
    "    df_train = drop_non_use_features(df_train)\n",
    "    df_test = drop_non_use_features(df_test)\n",
    "\n",
    "    X_train = fill_nan_mean(df_train)\n",
    "    X_test = fill_nan_mean(df_test)\n",
    "\n",
    "    X_train, X_test, Y_train = scale_values(X_train, X_test, Y_train)\n",
    "\n",
    "    return X_train, X_test, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c667c",
   "metadata": {},
   "source": [
    "# Data Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad9e9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/stores_train.csv\")\n",
    "df_test = pd.read_csv(\"data/stores_test.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "df_train = fix_outliers(df_train)\n",
    "df_train = fix_lat_lon(df_train) \n",
    "df_test = fix_lat_lon(df_test)\n",
    "\n",
    "# Features engineering\n",
    "X_train, X_test, Y_train = features_engineering(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c96ddcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =   [\n",
    "    'grunnkrets_id',\n",
    "    'SI_p1',\n",
    "    'SI_p2', \n",
    "    'SI_p3', \n",
    "    'SI_all',\n",
    "    'latxlat',\n",
    "    'population_density',\n",
    "    'ih_all_households',\n",
    "    'BS_closest_mangler_viktighetsniv√•',\n",
    "    'BS_closest_lokalt_knutepunkt',\n",
    "    'BS_closest_nasjonalt_knutepunkt',\n",
    "    'BS_closest_regionalt_knutepunkt',\n",
    "    'BS_closest_annen_viktig_holdeplass',\n",
    "    'encoded_lv3_desc',\n",
    "    'encoded_sales_channel_name',\n",
    "    'encoded_chain_name',\n",
    "    'encoded_mall_name',\n",
    "    'encoded_municipality_name',\n",
    "]\n",
    "\n",
    "X_train = keep_only_use_features(X_train, features)\n",
    "X_test = keep_only_use_features(X_test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d66b00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=180,\n",
    "    criterion='squared_error',\n",
    "    max_depth=None,\n",
    "    min_samples_split=14,\n",
    "    min_samples_leaf=11,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=300,\n",
    "    min_impurity_decrease=0.0,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    ccp_alpha=0.0,\n",
    "    max_samples=None,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMRegressor(\n",
    "    num_leaves=70,\n",
    "    max_depth=7, \n",
    "    n_estimators=2000,\n",
    "    min_data_in_leaf = 400,\n",
    "    learning_rate=0.05,\n",
    "    random_state=SEED,  \n",
    ")\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    objective='reg:squarederror', \n",
    "    n_estimators=300, \n",
    "    colsample_bytree=0.8958238323555624, \n",
    "    gamma=0.11909139052336326,\n",
    "    learning_rate=0.05983241782780355,\n",
    "    subsample=0.8889067727422637,\n",
    "    max_depth=5,\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a35496",
   "metadata": {},
   "source": [
    "# Model Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce79177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]\n",
    "\n",
    "NFOLDS = 5 # set number of folds for out-of-fold prediction\n",
    "kf = KFold(\n",
    "    n_splits=NFOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=SEED\n",
    ") # K-Folds cross-validator\n",
    "\n",
    "# oof = out of fold\n",
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    \"\"\"\n",
    "    Popular function on Kaggle.\n",
    "    \n",
    "    Trains a classifier on 4/5 of the training data and\n",
    "    predicts the rest (1/5). This procedure is repeated for all 5 folds,\n",
    "    thus we have predictions for all training set. This prediction is one\n",
    "    column of meta-data, later on used as a feature column by a meta-algorithm.\n",
    "    We predict the test part and average predictions across all 3 models.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    clf -- classifier\n",
    "    x_train -- 4/5 of training data\n",
    "    y_train -- corresponding labels\n",
    "    x_test -- all test data\n",
    "    \n",
    "    \"\"\"\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        x_tr = x_train.iloc[train_index, :]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train.iloc[test_index, :]\n",
    "\n",
    "        clf.fit(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d2ce41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=400, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=400\n"
     ]
    }
   ],
   "source": [
    "rf_oof_train, rf_oof_test = get_oof(rf_model, X_train, np.ravel(Y_train), X_test)\n",
    "lgbm_oof_train, lgbm_oof_test = get_oof(lgbm_model, X_train, np.ravel(Y_train), X_test)\n",
    "xgb_oof_train, xgb_oof_test = get_oof(xgb_model, X_train, np.ravel(Y_train), X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23604292",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((\n",
    "    rf_oof_train,\n",
    "    lgbm_oof_train,\n",
    "    xgb_oof_train,\n",
    "), axis=1)\n",
    "\n",
    "x_test = np.concatenate((\n",
    "    rf_oof_test,\n",
    "    lgbm_oof_test,\n",
    "    xgb_oof_test,\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfcdfff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "      <th>LGBM</th>\n",
       "      <th>XGB</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839384</td>\n",
       "      <td>1.266116</td>\n",
       "      <td>1.074493</td>\n",
       "      <td>1.278708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978402</td>\n",
       "      <td>1.433658</td>\n",
       "      <td>1.134399</td>\n",
       "      <td>1.394942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.818390</td>\n",
       "      <td>0.802136</td>\n",
       "      <td>0.928183</td>\n",
       "      <td>1.232971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.714633</td>\n",
       "      <td>0.793416</td>\n",
       "      <td>0.739758</td>\n",
       "      <td>1.012669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.619073</td>\n",
       "      <td>0.957293</td>\n",
       "      <td>0.835196</td>\n",
       "      <td>0.742568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12854</th>\n",
       "      <td>0.546683</td>\n",
       "      <td>0.387816</td>\n",
       "      <td>0.539882</td>\n",
       "      <td>0.036629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12855</th>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.532016</td>\n",
       "      <td>0.552822</td>\n",
       "      <td>0.449633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12856</th>\n",
       "      <td>0.719494</td>\n",
       "      <td>0.588411</td>\n",
       "      <td>0.743718</td>\n",
       "      <td>1.593563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12857</th>\n",
       "      <td>0.590086</td>\n",
       "      <td>0.623193</td>\n",
       "      <td>0.649426</td>\n",
       "      <td>0.666705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12858</th>\n",
       "      <td>0.213439</td>\n",
       "      <td>0.294728</td>\n",
       "      <td>0.350800</td>\n",
       "      <td>0.522183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12859 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             RF      LGBM       XGB     label\n",
       "0      0.839384  1.266116  1.074493  1.278708\n",
       "1      0.978402  1.433658  1.134399  1.394942\n",
       "2      0.818390  0.802136  0.928183  1.232971\n",
       "3      0.714633  0.793416  0.739758  1.012669\n",
       "4      0.619073  0.957293  0.835196  0.742568\n",
       "...         ...       ...       ...       ...\n",
       "12854  0.546683  0.387816  0.539882  0.036629\n",
       "12855  0.906838  0.532016  0.552822  0.449633\n",
       "12856  0.719494  0.588411  0.743718  1.593563\n",
       "12857  0.590086  0.623193  0.649426  0.666705\n",
       "12858  0.213439  0.294728  0.350800  0.522183\n",
       "\n",
       "[12859 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OOF predictions\n",
    "meta_df = pd.DataFrame(x_train, columns=[\n",
    "    'RF',\n",
    "    'LGBM',\n",
    "    'XGB',\n",
    "])\n",
    "meta_df['label'] = Y_train\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ef3fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_MODEL = LinearRegression()\n",
    "META_MODEL.fit(x_train, Y_train)\n",
    "Y_Pred = META_MODEL.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59bc106",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9e71780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>914206820-914239427-717245</td>\n",
       "      <td>1.912976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>916789157-916823770-824309</td>\n",
       "      <td>3.192157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>913341082-977479363-2948</td>\n",
       "      <td>5.810446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>889682582-889697172-28720</td>\n",
       "      <td>8.828748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>997991699-998006945-417222</td>\n",
       "      <td>4.795286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  predicted\n",
       "0  914206820-914239427-717245   1.912976\n",
       "1  916789157-916823770-824309   3.192157\n",
       "2    913341082-977479363-2948   5.810446\n",
       "3   889682582-889697172-28720   8.828748\n",
       "4  997991699-998006945-417222   4.795286"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test.store_id \n",
    "submission['predicted'] = np.asarray(10 ** Y_Pred - 1)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
