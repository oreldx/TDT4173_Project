{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f206da93",
   "metadata": {},
   "source": [
    "# Second Best Submission : erasmus_40\n",
    "## Public Score : 0.67531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4232380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn part\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler \n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "\n",
    "# other models\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# other\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "827d1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd57ed",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "babade38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_lat_lon(df):\n",
    "    df['lat_processed'] = df.lat * 11.112\n",
    "    df['lon_processed'] = df.lon * 6.4757\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d18bf",
   "metadata": {},
   "source": [
    "# Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0be5e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_year(df):\n",
    "    return df[df.year == 2016]\n",
    "\n",
    "\n",
    "def create_population_repartition(df_grunnkrets_house_pers):\n",
    "    tmp = df_grunnkrets_house_pers[[\n",
    "            'grunnkrets_id',\n",
    "            'couple_children_0_to_5_years',\n",
    "            'couple_children_6_to_17_years',\n",
    "            'couple_children_18_or_above',\n",
    "            'couple_without_children',\n",
    "            'single_parent_children_0_to_5_years',\n",
    "            'single_parent_children_6_to_17_years',\n",
    "            'single_parent_children_18_or_above',\n",
    "            'singles'\n",
    "        ]]\n",
    "    return tmp.rename({column: 'hp_' + column for column in tmp.columns if column != 'grunnkrets_id'}, axis=1)\n",
    "\n",
    "\n",
    "def create_income_repartition(df_grunnkrets_income_house):\n",
    "    tmp = df_grunnkrets_income_house[[\n",
    "            'grunnkrets_id',\n",
    "            'all_households',\n",
    "            'singles',\n",
    "            'couple_without_children',\n",
    "            'couple_with_children',\n",
    "            'other_households',\n",
    "            'single_parent_with_children'\n",
    "            ]]\n",
    "    return tmp.rename({column: 'ih_' + column for column in tmp.columns if column != 'grunnkrets_id'}, axis=1)\n",
    "\n",
    "\n",
    "def create_grunnkret_geodata(df_grunnkrets_stripped):\n",
    "    return df_grunnkrets_stripped[[\n",
    "                            'grunnkrets_id',\n",
    "                            'area_km2',\n",
    "                            'municipality_name'\n",
    "                            ]]\n",
    "\n",
    "\n",
    "def create_hierarchy(df_plaace_hierarchy):\n",
    "    return df_plaace_hierarchy[[\n",
    "                            'plaace_hierarchy_id',\n",
    "                            'lv1_desc',\n",
    "                            'lv2_desc',\n",
    "                            'lv3_desc'\n",
    "                            ]]\n",
    "\n",
    "\n",
    "def create_population_age(df_grunnkrets_age_dist):\n",
    "    df_grunnkrets_population = df_grunnkrets_age_dist.loc[:, ['grunnkrets_id']]\n",
    "    df_grunnkrets_population['total_population'] = df_grunnkrets_age_dist.iloc[:,2:92].sum(axis=1)\n",
    "    df_grunnkrets_population['youngs'] = df_grunnkrets_age_dist.iloc[:, 2:20].sum(axis=1)\n",
    "    df_grunnkrets_population['adults'] = df_grunnkrets_age_dist.iloc[:, 21:64].sum(axis=1)\n",
    "    df_grunnkrets_population['seniors'] = df_grunnkrets_age_dist.iloc[:, 65:92].sum(axis=1)\n",
    "    return df_grunnkrets_population\n",
    "\n",
    "\n",
    "def find_closest_bus_stop(df, df_bus_stops):\n",
    "    \"\"\"\n",
    "    Combine the training data with the bus stop data by finding :\n",
    "    - the closest bus stop from the store (create a feature the minimal distance then)\n",
    "    - the mean distance of every bus stop in 1km radius\n",
    "    for each category of bus stop\n",
    "    \"\"\"\n",
    "    df['lat_processed'] = df.lat_processed * 10\n",
    "    df['lon_processed'] = df.lon_processed * 10\n",
    "\n",
    "    categories = ['Mangler viktighetsniv√•',\n",
    "                  'Standard holdeplass',\n",
    "                  'Lokalt knutepunkt',\n",
    "                  'Nasjonalt knutepunkt',\n",
    "                  'Regionalt knutepunkt',\n",
    "                  'Annen viktig holdeplass']\n",
    "\n",
    "    new_bs_features = pd.DataFrame(df.store_id)\n",
    "\n",
    "    df_bus_tmp = df_bus_stops.loc[:, ['busstop_id']]\n",
    "    df_bus_tmp[['lon_processed', 'lat_processed']] = df_bus_stops['geometry'].str.extract(\n",
    "        r'(?P<lat>[0-9]*[.]?[0-9]+)\\s(?P<lon>[0-9]*[.]?[0-9]+)', expand=True)\n",
    "    df_bus_tmp['lon_processed'] = pd.to_numeric(df_bus_tmp['lon_processed']) * 6.4757 * 10  # value in km\n",
    "    df_bus_tmp['lat_processed'] = pd.to_numeric(df_bus_tmp['lat_processed']) * 11.112 * 10  # value in km\n",
    "\n",
    "    mat = cdist(df_bus_tmp[['lat_processed', 'lon_processed']], df[['lat_processed', 'lon_processed']], metric='euclidean')\n",
    "    correlation_dist = pd.DataFrame(mat, index=df_bus_tmp['busstop_id'], columns=df['store_id'])\n",
    "    new_bs_features = pd.merge(new_bs_features,\n",
    "                               pd.DataFrame(correlation_dist.min(),\n",
    "                                            columns=['BS_closest']),\n",
    "                               on='store_id', how='left')\n",
    "    new_bs_features = pd.merge(new_bs_features,\n",
    "                               pd.DataFrame(correlation_dist[correlation_dist < 1].mean(),\n",
    "                                            columns=['BS_mean_1km']),\n",
    "                               on='store_id', how='left')\n",
    "    new_bs_features = pd.merge(new_bs_features,\n",
    "                               pd.DataFrame(correlation_dist[correlation_dist < 0.5].count(),\n",
    "                                                columns=['number_BS_500m']), on='store_id')\n",
    "\n",
    "    for category in categories:\n",
    "        df_bus_tmp = df_bus_stops[df_bus_stops['importance_level'] == category].loc[:, ['busstop_id']]\n",
    "        df_bus_tmp[['lon_processed', 'lat_processed']] = df_bus_stops['geometry'].str.extract(\n",
    "                                                                r'(?P<lat>[0-9]*[.]?[0-9]+)\\s(?P<lon>[0-9]*[.]?[0-9]+)',\n",
    "                                                                expand=True)\n",
    "        df_bus_tmp['lon_processed'] = pd.to_numeric(df_bus_tmp['lon_processed']) * 6.4757 * 10    # value in km\n",
    "        df_bus_tmp['lat_processed'] = pd.to_numeric(df_bus_tmp['lat_processed']) * 11.112 * 10    # value in km\n",
    "\n",
    "        mat = cdist(df_bus_tmp[['lat_processed', 'lat_processed']], df[['lat_processed', 'lon_processed']], metric='euclidean')\n",
    "        correlation_dist = pd.DataFrame(mat, index=df_bus_tmp['busstop_id'], columns=df['store_id'])\n",
    "        new_bs_features = pd.merge(new_bs_features,\n",
    "                                   pd.DataFrame(correlation_dist.min(),\n",
    "                                                columns=['BS_closest_' + category.lower().replace(' ', '_')]),\n",
    "                                   on='store_id', how='left')\n",
    "        new_bs_features = pd.merge(new_bs_features,\n",
    "                                   pd.DataFrame(correlation_dist[correlation_dist < 1].mean(),\n",
    "                                                columns=['BS_mean_1km_'+category.lower().replace(' ', '_')]),\n",
    "                                   on='store_id', how='left')\n",
    "    return new_bs_features.fillna(0)\n",
    "\n",
    "\n",
    "def fix_municipalities(current_df):\n",
    "    # Get the rows with missing municipality\n",
    "    df_missing_mun = current_df[current_df[\"municipality_name\"].isna()]\n",
    "    # Create a copy of the current df and drop row where mun = NaN + Reset index\n",
    "    current_df_copy = current_df.copy().dropna(subset=['municipality_name'])\n",
    "    current_df_copy = current_df_copy.reset_index(drop=True)\n",
    "    # For each missing municipality\n",
    "    for index, row in df_missing_mun.iterrows():\n",
    "        # Create a df with the the difference with the loc of the current store and all the others stores\n",
    "        tmp_df = pd.concat([current_df_copy.loc[:, [\"lat_processed\"]] - row.lat_processed,\n",
    "                            current_df_copy.loc[:, [\"lon_processed\"]] - row.lon_processed], axis=1)\n",
    "        # Find the idx of the one with the smallest error (the closest from the other)\n",
    "        idx = np.argmin(np.linalg.norm(tmp_df.to_numpy(), axis=1))\n",
    "        # Retrieve the municipality of the closest one and input it in the missing one\n",
    "        current_df.loc[index, \"municipality_name\"] = current_df_copy.loc[idx, \"municipality_name\"]\n",
    "    return current_df\n",
    "\n",
    "\n",
    "def keep_only_use_features(current_df, features):\n",
    "    return current_df.loc[:, features]\n",
    "\n",
    "\n",
    "def drop_non_use_features(current_df):\n",
    "    return current_df.drop([\n",
    "                        'year',\n",
    "                        'store_id',\n",
    "                        'store_name',\n",
    "                        'address',\n",
    "                        'sales_channel_name',\n",
    "                        'chain_name',\n",
    "                        'mall_name',\n",
    "                        'plaace_hierarchy_id',\n",
    "                        'lv1_desc',\n",
    "                        'lv2_desc',\n",
    "                        'lv3_desc',\n",
    "                        'municipality_name',\n",
    "                        ], axis=1)\n",
    "\n",
    "\n",
    "def label_uniformier(array_train, array_test):\n",
    "    \"\"\"\n",
    "    Take the unique values from the train and test part to combine it in a single array.\n",
    "    Useful to fit the label encoder and don't do a mess during the transform (previously fit_transform that was confusing)\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = np.asarray(list(array_train.unique()) + list(set(array_test.unique()) - set(array_train.unique())))\n",
    "    label_encoder.fit(labels)\n",
    "    return label_encoder\n",
    "\n",
    "\n",
    "def encode_feature(df_train, df_test, feature_name):\n",
    "    le = label_uniformier(df_train[feature_name], df_test[feature_name])\n",
    "    df_train['encoded_' + feature_name] = le.transform(df_train[feature_name])\n",
    "    df_test['encoded_' + feature_name] = le.transform(df_test[feature_name])\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def fill_nan_mean(current_df):\n",
    "    return current_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "\n",
    "\n",
    "def store_secret_feature(df):\n",
    "    tmp_df = df.loc[:, ['store_id', ]]\n",
    "    tmp_df[['SI_p1', 'SI_p2', 'SI_p3']] = df['store_id'].str.extract(r'(?P<p1>[0-9]+)-+(?P<p2>[0-9]+)-+(?P<p3>[0-9]+)', expand=True)\n",
    "    tmp_df['SI_all'] = tmp_df[['SI_p1', 'SI_p2', 'SI_p3']].stack().groupby(level=0).apply(''.join)\n",
    "    tmp_df[['SI_p1', 'SI_p2', 'SI_p3']] = tmp_df[['SI_p1', 'SI_p2', 'SI_p3']].apply(pd.to_numeric)\n",
    "    tmp_df['SI_all'] = tmp_df['SI_all'].astype('float')\n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "def lat_lon_precisionless(df):\n",
    "    tmp_df = df.loc[:, ['store_id', 'lat', 'lon']]\n",
    "    tmp_df[['lat', 'lon']] = tmp_df[['lat', 'lon']]\n",
    "    tmp_df[['lat_reduced', 'lon_reduced']] = tmp_df[['lat', 'lon']].astype('int')\n",
    "    return tmp_df[['store_id', 'lat_reduced', 'lon_reduced']]\n",
    "\n",
    "\n",
    "def scale_values(df_train, df_test, Y_train):\n",
    "    scaler = StandardScaler()\n",
    "    df_train[df_train.columns] = scaler.fit_transform(df_train)\n",
    "    df_test[df_train.columns] = scaler.transform(df_test)\n",
    "\n",
    "    Y_train = np.log10(Y_train + 1)\n",
    "\n",
    "    return df_train, df_test, Y_train\n",
    "\n",
    "\n",
    "def extract_revenue(df_train):\n",
    "    return df_train.loc[:, ['revenue', ]], df_train.drop(['revenue'], axis=1)\n",
    "\n",
    "\n",
    "def features_engineering(df_train, df_test):\n",
    "    df_bus_stops = pd.read_csv(\"data/busstops_norway.csv\")\n",
    "    df_grunnkrets_age_dist = pd.read_csv(\"data/grunnkrets_age_distribution.csv\")\n",
    "    df_grunnkrets_house_pers = pd.read_csv(\"data/grunnkrets_households_num_persons.csv\")\n",
    "    df_grunnkrets_income_house = pd.read_csv(\"data/grunnkrets_income_households.csv\")\n",
    "    df_grunnkrets_stripped = pd.read_csv(\"data/grunnkrets_norway_stripped.csv\")\n",
    "    df_plaace_hierarchy = pd.read_csv(\"data/plaace_hierarchy.csv\")\n",
    "\n",
    "    df_grunnkrets_stripped = fix_year(df_grunnkrets_stripped)\n",
    "    df_grunnkrets_age_dist = fix_year(df_grunnkrets_age_dist)\n",
    "    df_grunnkrets_house_pers = fix_year(df_grunnkrets_house_pers)\n",
    "    df_grunnkrets_income_house = fix_year(df_grunnkrets_income_house)\n",
    "\n",
    "    df_train = pd.merge(df_train, store_secret_feature(df_train), how=\"left\", on=\"store_id\")\n",
    "    df_test = pd.merge(df_test, store_secret_feature(df_test), how=\"left\", on=\"store_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, lat_lon_precisionless(df_train), how=\"left\", on=\"store_id\")\n",
    "    df_test = pd.merge(df_test, lat_lon_precisionless(df_test), how=\"left\", on=\"store_id\")\n",
    "    df_train['latxlat'] = df_train['lat_reduced']*df_train['lon_reduced']\n",
    "    df_test['latxlat'] = df_test['lat_reduced']*df_test['lon_reduced']\n",
    "\n",
    "    df_train = pd.merge(df_train, create_population_repartition(df_grunnkrets_house_pers), how=\"left\", on=\"grunnkrets_id\")\n",
    "    df_test = pd.merge(df_test, create_population_repartition(df_grunnkrets_house_pers), how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, create_income_repartition(df_grunnkrets_income_house), how=\"left\",on=\"grunnkrets_id\")\n",
    "    df_test = pd.merge(df_test, create_income_repartition(df_grunnkrets_income_house), how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, create_hierarchy(df_plaace_hierarchy), how=\"left\", on=\"plaace_hierarchy_id\")\n",
    "    df_test = pd.merge(df_test, create_hierarchy(df_plaace_hierarchy), how=\"left\", on=\"plaace_hierarchy_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, create_grunnkret_geodata(df_grunnkrets_stripped), how=\"left\", on=\"grunnkrets_id\")\n",
    "    df_test = pd.merge(df_test, create_grunnkret_geodata(df_grunnkrets_stripped), how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "    df_train = pd.merge(df_train, create_population_age(df_grunnkrets_age_dist), how=\"left\", on=\"grunnkrets_id\")\n",
    "    df_test = pd.merge(df_test, create_population_age(df_grunnkrets_age_dist), how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "    df_train[\"population_density\"] = df_train[\"total_population\"] / df_train[\"area_km2\"]\n",
    "    df_test[\"population_density\"] = df_test[\"total_population\"] / df_test[\"area_km2\"]\n",
    "\n",
    "    df_train = pd.merge(df_train, find_closest_bus_stop(df_train, df_bus_stops), how=\"left\", on=\"store_id\")\n",
    "    df_test = pd.merge(df_test, find_closest_bus_stop(df_test, df_bus_stops), how=\"left\", on=\"store_id\")\n",
    "\n",
    "    df_train['mall_name'] = df_train['mall_name'].fillna('0')\n",
    "    df_test['mall_name'] = df_test['mall_name'].fillna('0')\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'mall_name')\n",
    "\n",
    "    df_train['chain_name'] = df_train['chain_name'].fillna('0')\n",
    "    df_test['chain_name'] = df_test['chain_name'].fillna('0')\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'chain_name')\n",
    "\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'sales_channel_name')\n",
    "\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'lv3_desc')\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'lv2_desc')\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'lv1_desc')\n",
    "\n",
    "    df_train = fix_municipalities(df_train)\n",
    "    df_test = fix_municipalities(df_test)\n",
    "    df_train, df_test = encode_feature(df_train, df_test, 'municipality_name')\n",
    "\n",
    "    Y_train, df_train = extract_revenue(df_train)\n",
    "\n",
    "    df_train = drop_non_use_features(df_train)\n",
    "    df_test = drop_non_use_features(df_test)\n",
    "\n",
    "    X_train = fill_nan_mean(df_train)\n",
    "    X_test = fill_nan_mean(df_test)\n",
    "\n",
    "    X_train, X_test, Y_train = scale_values(X_train, X_test, Y_train)\n",
    "\n",
    "    return X_train, X_test, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a2d77",
   "metadata": {},
   "source": [
    "# Data Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00098d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/stores_train.csv\")\n",
    "df_test = pd.read_csv(\"data/stores_test.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "df_train = fix_lat_lon(df_train) \n",
    "df_test = fix_lat_lon(df_test)\n",
    "\n",
    "# Features engineering\n",
    "X_train, X_test, Y_train = features_engineering(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80dc5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =   [\n",
    "    'grunnkrets_id',\n",
    "    'SI_p1',\n",
    "    'SI_p2', \n",
    "    'SI_p3', \n",
    "    'SI_all',\n",
    "    'population_density',\n",
    "    'ih_all_households',\n",
    "    'BS_closest_mangler_viktighetsniv√•',\n",
    "    'BS_closest_lokalt_knutepunkt',\n",
    "    'BS_closest_nasjonalt_knutepunkt',\n",
    "    'BS_closest_regionalt_knutepunkt',\n",
    "    'BS_closest_annen_viktig_holdeplass',\n",
    "    'encoded_lv3_desc',\n",
    "    'encoded_sales_channel_name',\n",
    "    'encoded_chain_name',\n",
    "    'encoded_mall_name',\n",
    "    'encoded_municipality_name',\n",
    "    'ih_singles', \n",
    "    'ih_couple_without_children',\n",
    "    'ih_couple_with_children', \n",
    "    'ih_other_households',\n",
    "    'ih_single_parent_with_children',\n",
    "]\n",
    "\n",
    "X_train = keep_only_use_features(X_train, features)\n",
    "X_test = keep_only_use_features(X_test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a2fb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror', \n",
    "    n_estimators=300, \n",
    "    colsample_bytree=0.8958238323555624, \n",
    "    gamma=0.11909139052336326,\n",
    "    learning_rate=0.05983241782780355,\n",
    "    subsample=0.8889067727422637,\n",
    "    max_depth=5,\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f2840",
   "metadata": {},
   "source": [
    "# Model Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86830bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)\n",
    "Y_Pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efcad5",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e021b7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>914206820-914239427-717245</td>\n",
       "      <td>1.830829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>916789157-916823770-824309</td>\n",
       "      <td>3.399110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>913341082-977479363-2948</td>\n",
       "      <td>5.715342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>889682582-889697172-28720</td>\n",
       "      <td>11.634244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>997991699-998006945-417222</td>\n",
       "      <td>6.326633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  predicted\n",
       "0  914206820-914239427-717245   1.830829\n",
       "1  916789157-916823770-824309   3.399110\n",
       "2    913341082-977479363-2948   5.715342\n",
       "3   889682582-889697172-28720  11.634244\n",
       "4  997991699-998006945-417222   6.326633"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test.store_id \n",
    "submission['predicted'] = np.asarray(10 ** Y_Pred - 1)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
